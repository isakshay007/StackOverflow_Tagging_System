# evaluate_model.py

import time
import joblib
import pandas as pd
from sklearn.metrics import (
    precision_score, recall_score, f1_score
)
from sklearn.preprocessing import MultiLabelBinarizer

def load_test_data(csv_path, mlb_classes):
    df = pd.read_csv(csv_path)

    # è¿™é‡Œç”¨ df['text'] ä»£æ›¿ 'Title' + 'Question'
    # use df['text'] to substitute 'Title' + 'Question'
    df['combined_text'] = df['text'].fillna('')

    # æŠŠ tags åˆ—ç”¨ ',' æ‹†åˆ† â†’ å†è¿‡æ»¤åªä¿ç•™æ¨¡å‹è§è¿‡çš„
    # split the tags column by ',' and filter only the tags seen by the model
    def filter_tags(x):
        raw = [t.strip() for t in x.split(',') if t.strip()]
        return [t for t in raw if t in mlb_classes]

    df['tag_list'] = df['tags'].apply(filter_tags)
    df = df[df['tag_list'].map(len) > 0]
    return df


if __name__ == "__main__":
    start_time = time.time()

    # 1. åŠ è½½å·²è®­ç»ƒå¥½çš„æ¨¡å‹
    # load the trained model
    pipeline = joblib.load("tagging_model.pkl")
    mlb = joblib.load("tagging_mlb.pkl")

    # 2. è¯»å–ä½ çš„æµ‹è¯•/éªŒè¯æ•°æ®
    # read your test/validation data
    test_data_path = "val_split.csv"  # æˆ–è€…åˆ«çš„ CSV
    df_test = load_test_data(test_data_path, mlb.classes_)

    # 3. æ„é€  X,y
    # construct X,y
    X_test = df_test['combined_text']
    # æŠŠæ ‡ç­¾è½¬æˆ 0/1
    y_test = mlb.transform(df_test['tag_list'])

    # 4. æ¨¡å‹é¢„æµ‹
    # model prediction
    y_pred = pipeline.predict(X_test)

    # 5. è®¡ç®—æŒ‡æ ‡ï¼ˆmicro-averageï¼‰
    # calculate metrics (micro-average)
    p = precision_score(y_test, y_pred, average='micro', zero_division=0)
    r = recall_score(y_test, y_pred, average='micro', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='micro', zero_division=0)

    elapsed = time.time() - start_time

    print("\nâœ… Evaluation Complete!")
    print(f"ğŸ•’ Time Taken: {elapsed:.2f} seconds")
    print(f"ğŸ“Œ Precision: {p:.4f}")
    print(f"ğŸ“Œ Recall:    {r:.4f}")
    print(f"ğŸ“Œ F1 Score:  {f1:.4f}\n")
