import time
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    precision_score,
    recall_score,
    f1_score,
)
import joblib
from collections import Counter

def load_and_preprocess_data(data_path, min_freq=2):
    df = pd.read_csv(data_path, encoding="utf-8")

    if 'Title' not in df.columns or 'Question' not in df.columns or 'Tags' not in df.columns:
        raise ValueError("CSV æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: Title, Question, Tags")

    # merge Title and Question
    df['combined_text'] = df['Title'].fillna('') + ' ' + df['Question'].fillna('')
    # spilt tags
    df['tag_list'] = df['Tags'].apply(lambda x: [t.strip() for t in x.split(',') if t.strip()])

    # how many times each 
    counter = Counter()
    for tags in df['tag_list']:
        counter.update(tags)

    # åªä¿ç•™å‡ºç°æ¬¡æ•° >= min_freq çš„æ ‡ç­¾
    # only save the one which have frequency >= min_freq
    valid_tags = set([t for t, freq in counter.items() if freq >= min_freq])
    df['tag_list'] = df['tag_list'].apply(lambda tags: [t for t in tags if t in valid_tags])
    df = df[df['tag_list'].map(len) > 0]

    return df

def build_and_train_model(df):
    X = df['combined_text']
    y_raw = df['tag_list']

    mlb = MultiLabelBinarizer()
    y = mlb.fit_transform(y_raw)

    # æ‹†æˆè®­ç»ƒ / æµ‹è¯•é›†
    # split into train / test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # æ„å»º Pipeline
    # build pipeline
    pipeline = Pipeline([
        ('tfidf', TfidfVectorizer()),
        ('clf', OneVsRestClassifier(LogisticRegression(max_iter=1000)))
    ])

    # è®­ç»ƒ
    # train
    pipeline.fit(X_train, y_train)

    return pipeline, mlb, X_test, y_test

if __name__ == "__main__":
    import time

    start_time = time.time()

    # 1. åŠ è½½å¹¶é¢„å¤„ç†
    # load and preprocess
    data_path = "data/stackoverflow_data.csv"  # ä¿®æ”¹æˆä½ çš„æ•°æ®è·¯å¾„
    df = load_and_preprocess_data(data_path, min_freq=2)

    # 2. è®­ç»ƒ
    # train
    model, mlb, X_test, y_test = build_and_train_model(df)

    # 3. è¯„ä¼°
    # evaluate
    y_pred = model.predict(X_test)

    # è¿™é‡Œæˆ‘ä»¬åªè®¡ç®— micro-average çš„ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1
    # here we only calculate micro-average precision, recall, F1
    p_micro = precision_score(y_test, y_pred, average='micro', zero_division=0)
    r_micro = recall_score(y_test, y_pred, average='micro', zero_division=0)
    f1_micro = f1_score(y_test, y_pred, average='micro', zero_division=0)

    # 4. ç»“æŸæ—¶é—´
    #   end time
    end_time = time.time()
    time_taken = end_time - start_time


    print("\nâœ… Evaluation Complete!")
    print(f"ğŸ•’ Time Taken: {time_taken:.2f} seconds")
    print(f"ğŸ“Œ Precision: {p_micro:.4f}")
    print(f"ğŸ“Œ Recall:    {r_micro:.4f}")
    print(f"ğŸ“Œ F1 Score:  {f1_micro:.4f}\n")

    # no need to do it now å†æ‰“å° Hamming Loss, classification_report
    #h_loss = hamming_loss(y_test, y_pred)
    #print(f"Hamming Loss: {h_loss:.4f}")
    #print("\nğŸ“Š Classification Report:\n")
    #print(classification_report(y_test, y_pred, zero_division=0))

    # ä¿å­˜æ¨¡å‹
    joblib.dump(model, "tagging_model.pkl")
    joblib.dump(mlb, "tagging_mlb.pkl")
    print("æ¨¡å‹å’Œæ ‡ç­¾ç¼–ç å™¨å·²ä¿å­˜ã€‚")
